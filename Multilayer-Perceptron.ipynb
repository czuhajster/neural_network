{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534af2af",
   "metadata": {},
   "source": [
    "# Backpropagation Neural Network\n",
    "\n",
    "Backpropagation directs the weight changes down the gradient of steepest descent (hence Gradient Descent) of the error function and adjusts the weights according to a learning parameter which is set by the user.\n",
    "\n",
    "This is possible because of the continuous nature and differentiability of the activation functions in the neurons.\n",
    "\n",
    "## Momentum\n",
    "\n",
    "## Bold Driver\n",
    "\n",
    "## Annealing\n",
    "\n",
    "## Weight Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd018c2",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "\n",
    "Gradient of the error function\n",
    "\n",
    "- $\\frac{\\delta E}{\\delta w_{ij}}$ is the rate of change of Error $E$ with respect to a weight $w_{ij}$. This is the gradient of the error function.\n",
    "\n",
    "## Updating weights\n",
    "\n",
    "When updating weights, we are **subtracting** the gradient of the error function ($\\times \\rho$):\n",
    "\n",
    "$$\n",
    "w^*_{i,j} = w_{i,j} + \\rho \\delta_j u_i\n",
    "$$\n",
    "\n",
    "\n",
    "![Gradient of the Error function](figures/error-gradient.png)\n",
    "\n",
    "\n",
    "Too small weights - stuck in local minima\n",
    "\n",
    "- we start with random weights and biases\n",
    "\n",
    "- searching multidiensional spaces result less often in being caught in local minima.\n",
    "\n",
    "## When do we stop learning?\n",
    "\n",
    "\n",
    "- We tend to stop learning when the error of an independent validation set increases.\n",
    "\n",
    "- Every $x$ number of epochs, we test it against this unseen validation set.\n",
    "\n",
    "\n",
    "### Symmetry in Weight Space\n",
    "\n",
    "Network with $M$ hidden nodes exhbits symetry by a factor of $M!2^M$.\n",
    "\n",
    "For example, if we have 3 hidden nodes, we will have $3!2^3 = 48$ the same global minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c53ee",
   "metadata": {},
   "source": [
    "## Measuring Performance\n",
    "\n",
    "### Mean Squared Error\n",
    "\n",
    "$$\n",
    "MSE = \\frac{\\sum(O - M)^2}{n}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $O$ is the observed value\n",
    "- $M$ is the modelled value\n",
    "- $n$ is the number of example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b98555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88baf982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(observed: np.ndarray, modelled: np.ndarray):\n",
    "    \"\"\"Calculate the Mean Squared Error.\n",
    "    \n",
    "    Args:\n",
    "        observed: Array of the observed values.\n",
    "        modelled: Array of the modelled values.\n",
    "        \n",
    "    Returns:\n",
    "        The Mean Squared Error for the given arrays.\n",
    "    \"\"\"\n",
    "    return np.sum(np.power(observed - modelled, 2)) / len(observed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93745d66",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error\n",
    "\n",
    "- no upper bound\n",
    "- for perfect model, RMSE = 0\n",
    "- records real unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e19047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(observed: np.ndarray, modelled: np.ndarray):\n",
    "    \"\"\"Calculates the Root Mean Squared Error.\n",
    "     \n",
    "    Args:\n",
    "        observed: Array of the observed values.\n",
    "        modelled: Array of the modelled values.\n",
    "        \n",
    "    Returns:\n",
    "        The Root Mean Squared Error for the given arrays.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(np.power(observed - modelled, 2)) / len(observed))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0250a9f",
   "metadata": {},
   "source": [
    "### Mean Squared relative Error\n",
    "\n",
    "- error relative to the observed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20913de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def msre(observed: np.ndarray, modelled: np.ndarray):\n",
    "    \"\"\"Calculates the Mean Squared Relative Error.\n",
    "     \n",
    "    Args:\n",
    "        observed: Array of the observed values.\n",
    "        modelled: Array of the modelled values.\n",
    "        \n",
    "    Returns:\n",
    "        The Mean Squared Relative Error for the given arrays.\n",
    "    \"\"\"\n",
    "    return np.sum(np.power((modelled - observed) / observed, 2)) / len(observed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3051001d",
   "metadata": {},
   "source": [
    "### Coefficient of Efficiency\n",
    "\n",
    "- +1 represents perfect model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3391a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce(observed: np.ndarray, modelled: np.ndarray):\n",
    "    \"\"\"Calculates the Coefficient of Efficiency.\n",
    "     \n",
    "    Args:\n",
    "        observed: Array of the observed values.\n",
    "        modelled: Array of the modelled values.\n",
    "        \n",
    "    Returns:\n",
    "        The Coefficient of Efficiency for the given arrays.\n",
    "    \"\"\"\n",
    "    return 1 - (np.sum(np.power(modelled - observed, 2)) / np.sum(np.power(observed - np.mean(observed), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b126020",
   "metadata": {},
   "source": [
    "### R-Squared - Coefficient of Determination\n",
    "\n",
    "- measures the coincidence of the shape\n",
    "range from 0 ro 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsqr(observed: np.ndarray, modelled: np.ndarray):\n",
    "    \"\"\"Calculates the Coefficient of Determination - .\n",
    "     \n",
    "    Args:\n",
    "        observed: Array of the observed values.\n",
    "        modelled: Array of the modelled values.\n",
    "        \n",
    "    Returns:\n",
    "        The Mean Squared Error for the given arrays.\n",
    "    \"\"\"\n",
    "    dividend = np.sum(np.multiply(observed - observed.mean(), modelled - modelled.mean()))\n",
    "    divisor = np.sqrt(np.multiply(np.sum(np.power(observed - observed.mean(), 2)), np.sum(np.power(modelled - modelled.mean(), 2))))\n",
    "    return np.power(dividend / divisor, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd9ea93",
   "metadata": {},
   "source": [
    "## Number of Hidden Nodes\n",
    "\n",
    "- It is not fixed\n",
    "- The general rule of thumb is, having $n$ inputs, to try $\\frac{n}{2}$ to $2n$ hidden nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29540ff1",
   "metadata": {},
   "source": [
    "## Things to manipulate\n",
    "\n",
    "- number of hidden nodes\n",
    "- step size\n",
    "- activation function (optionally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee7cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron():\n",
    "    \"\"\"Object representing perceptron with two inputs.\n",
    "\n",
    "    Attributes:\n",
    "        e: A training set.\n",
    "        w0: Bias weight.\n",
    "        w1: The weight of the first input.\n",
    "        w2: The weight of the second input.\n",
    "        epochs: Number of epochs before stabilisation.\n",
    "    \"\"\"\n",
    "    def __init__(self, e, activation_function = lambda s: 1 if s > 0 else -1):\n",
    "        '''Initialises Perceptron object.'''\n",
    "        self.w0 = 0\n",
    "        self.w1 = 0\n",
    "        self.w2 = 0\n",
    "        self.e = e\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Trains perceptron.\"\"\"\n",
    "        self.epochs = 1\n",
    "        stable = False\n",
    "        while not stable:\n",
    "            stable = True\n",
    "            for example in self.e:\n",
    "                print(example)\n",
    "                if self.classify(example[1], example[2]) == example[3]:\n",
    "                    pass\n",
    "                else:\n",
    "                    self.w0 += example[3] * example[0]\n",
    "                    self.w1 += example[3] * example[1]\n",
    "                    self.w2 += example[3] * example[2]\n",
    "                    stable = False\n",
    "            if not stable:\n",
    "                self.epochs += 1\n",
    "\n",
    "    def classify(self, x1, x2):\n",
    "        \"\"\"Classifies an object.\"\"\"\n",
    "        s = (self.w1 * x1) + (self.w2 * x2) + self.w0\n",
    "        return self.activation_function(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94222063",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "- Every node on hidden layers and output layer has an activation function.\n",
    "\n",
    "\n",
    "\n",
    "- We might have **linear activation function in the output layer** instead of using sigmoid or tanh.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c951ed81",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "#### First Order Derivative\n",
    "\n",
    "We also need to calculate the first order differential of the function.\n",
    "\n",
    "$$\n",
    "f'(x) = f(x)(1 - f(x))\n",
    "$$\n",
    "\n",
    "$$\n",
    "f'(S_j) = u_j (1 - u_j)\n",
    "$$\n",
    "\n",
    "#### Delta Values\n",
    "\n",
    "- $\\delta_j = (C - u_O) f'(S_O)$ O is the output node.\n",
    "- $\\delta_j = w_{j,O} \\delta_O f'(S_j)$ for hidden layer nodes.\n",
    "\n",
    "\n",
    "\n",
    "#### Notes\n",
    "\n",
    "Sigmoid func has gradient problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a10c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"Represents Sigmoid activation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialises a sigmoid object.\"\"\"\n",
    "        self.vectorised_func = np.vectorize(self.func)\n",
    "        self.vectorised_der = np.vectorize(self.der)\n",
    "        \n",
    "    def func(self, x):\n",
    "        \"\"\"Calculates output of the Sigmoid function.\"\"\"\n",
    "        return 1 / (1 + np.e ** (-x))\n",
    "    \n",
    "    def der(self, x):\n",
    "        \"\"\"Calculates output of the derivative of the Sigmoid function.\n",
    "        \"\"\"\n",
    "        return self.func(x) * (1 - self.func(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be4c0f",
   "metadata": {},
   "source": [
    "### Tanh - Hyperbolic tangent\n",
    "\n",
    "$$\n",
    "tanh x = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "#### First Order Derivative\n",
    "\n",
    "$$\n",
    "tanh' x = 1 - tanh^2 x\n",
    "$$\n",
    "\n",
    "$$\n",
    "f'(S_j) = 1 - u^2_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd9a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    \"\"\"Represents tanh activation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialises a tanh object.\"\"\"\n",
    "        self.vectorised_func = np.vectorize(self.func)\n",
    "        self.vectorised_der = np.vectorize(self.der)\n",
    "    \n",
    "    def func(self, x):\n",
    "        \"\"\"Calculates output of the tanh function.\"\"\"\n",
    "        return (np.e ** x - np.e ** (-x)) / (np.e ** x + np.e ** (-x))\n",
    "    \n",
    "    def der(self, x):\n",
    "        \"\"\"Calculates output of the derivative of the tanh function.\n",
    "        \"\"\"\n",
    "        return 1 - self.func(x) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264882be",
   "metadata": {},
   "source": [
    "### ReLU - Rectified Linear Unit\n",
    "\n",
    "$$\n",
    "f(x) = \\begin{cases}\n",
    "    x & \\text{if $x > 0$}\\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### First Order Derivative\n",
    "\n",
    "$$\n",
    "f'(x) = \\begin{cases}\n",
    "    1 & \\text{if $x > 0$}\\\\\n",
    "    0 & \\text{if $x < 0$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The derivative of the ReLU function is not defined at $x = 0$. However, when implementing the derivation of ReLU we tend to define $\\frac{d}{dx} \\text{ReLU}(0) = 0$, instead of returning undefined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35dab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    \"\"\"Represents ReLU activation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialises a Relu object.\"\"\"\n",
    "        self.vectorised_func = np.vectorize(self.func)\n",
    "        self.vectorised_der = np.vectorize(self.der)\n",
    "    \n",
    "    def func(self, x):\n",
    "        \"\"\"Calculates output of the ReLU function.\"\"\"\n",
    "        if x > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def der(self, x):\n",
    "        \"\"\"Calculates output of the derivative of the ReLU function.\n",
    "        \"\"\"\n",
    "        if x > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a32658",
   "metadata": {},
   "source": [
    "### Leaky ReLU - Leaky Rectified Linear Unit\n",
    "\n",
    "$$\n",
    "f(x) = \\begin{cases}\n",
    "    x & \\text{if $x > 0$}\\\\\n",
    "    0.01x & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### First Order Derivative\n",
    "\n",
    "$$\n",
    "f'(x) = \\begin{cases}\n",
    "    1 & \\text{if $x > 0$}\\\\\n",
    "    0.01 & \\text{if $x < 0$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The derivative of the Leaky ReLU function is not defined at $x = 0$. However, when implementing the derivation of Leaky ReLU we tend to define $\\frac{d}{dx} \\text{LeakyReLU}(0) = 0.01$, instead of returning undefined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89455365",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyRelu:\n",
    "    \"\"\"Represents Leaky ReLU activation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialises a LeakyRelu object.\"\"\"\n",
    "        self.vectorised_func = np.vectorize(self.func)\n",
    "        self.vectorised_der = np.vectorize(self.der)\n",
    "    \n",
    "    def func(self, x):\n",
    "        \"\"\"Calculates output of the Leaky ReLU function.\"\"\"\n",
    "        if x > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return 0.01 * x\n",
    "    \n",
    "    def der(self, x):\n",
    "        \"\"\"Calculates output of the derivative of the Leaky ReLU function.\n",
    "        \"\"\"\n",
    "        if x > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966f104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def destandardise(x: np.ndarray, max_value: float, min_value: float):\n",
    "    \"\"\"Destandardises data using minimum and maximum values.\n",
    "    \n",
    "    Args:\n",
    "    x: A numpy.ndarray instance of standardised data.\n",
    "    max_value: A maximum value for the destandardisation formula.\n",
    "    min_value: A minimum value for the destandardisation formula.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray.\n",
    "    \"\"\"\n",
    "    return ((x - 0.1) * (max_value - min_value)) / 0.8 + min_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11feeabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backpropagation:\n",
    "    \"\"\"Backpropagation algorithm for training neural networks.\n",
    "    \n",
    "    Attributes:\n",
    "        neural_network: NeuralNetwork instance being trained.\n",
    "        epochs: Number of epochs the nerual network has gone through during the training.\n",
    "        previous_validation_error: Previous error (MSE) on the validation set.\n",
    "        previous_training_error: Previous error (MSE) on the training set.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        neural_network\n",
    "    ):\n",
    "        \"\"\"Initialises backpropagation object.\"\"\"\n",
    "        self.neural_network = neural_network\n",
    "        self.epochs = 0\n",
    "        self.previous_validation_error = np.inf\n",
    "        self.previous_training_error = np.inf\n",
    "        self.validation_errors = []\n",
    "        self.training_errors = []\n",
    "        self.tested_at_epochs = []\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        training_set: np.ndarray,\n",
    "        validation_set: np.ndarray,\n",
    "        validation_frequency: int,\n",
    "        learning_rate: float,\n",
    "        epoch_limit: int\n",
    "    ):\n",
    "        \"\"\"Trains NeuralNetwork instance using Backpropagation.\n",
    "        \n",
    "        Args:\n",
    "        training_set: Set that instance is trained on.\n",
    "        validation_set: Set the instance is tested on during training.\n",
    "        validation_frequency: Frequency of testing on validation set.\n",
    "            Expressed in epochs.\n",
    "        learning_rate: Learning rate.\n",
    "        epoch_limit: The maximum number of epochs to train the instace for.\n",
    "        \"\"\"\n",
    "        # Loop through the training set.\n",
    "        for i in range(epoch_limit):\n",
    "            # Annealing\n",
    "            #learning_rate = self.simulated_annealing(\n",
    "            #   start_rate=learning_rate,\n",
    "            #   end_rate=0.01,\n",
    "            #    epoch_limit=epoch_limit,\n",
    "            #   epochs_passed=self.epochs\n",
    "            #)\n",
    "            self.epochs = i + 1\n",
    "            for training_example in training_set:\n",
    "                # Split individual examples into inputs (item) and label (c).\n",
    "                item, c = np.hsplit(training_example, [training_set.shape[1] - 1])\n",
    "                item = item.reshape(1, -1)\n",
    "                c = c.reshape(1, -1)\n",
    "                self.forward_pass(item)\n",
    "                self.backward_pass(c, self.epochs, learning_rate)\n",
    "                self.update_weights(item, learning_rate)\n",
    "            # Bold driver.\n",
    "            #if (i + 1) % 1000 == 0:\n",
    "            #   # Test against the training set.\n",
    "            #    observed, predicted = self.neural_network.test(training_set)\n",
    "            #    current_training_error = mse(observed, predicted)\n",
    "            #    learning_rate = self.bold_driver(\n",
    "            #        current_training_error,\n",
    "            #        learning_rate\n",
    "            #    )\n",
    "            #   self.previous_training_error = current_training_error\n",
    "            # Test on validation set.\n",
    "            if (i + 1) % validation_frequency == 0:\n",
    "                observed_test, predicted_test = self.neural_network.test(training_set)\n",
    "                current_test_error = mse(observed_test, predicted_test)\n",
    "                self.training_errors.append(current_test_error)\n",
    "                \n",
    "                observed, predicted = self.neural_network.test(validation_set)\n",
    "                current_validation_error = mse(observed, predicted)\n",
    "                self.validation_errors.append(current_validation_error)\n",
    "                self.tested_at_epochs.append(self.epochs)\n",
    "                print(f\"Current Validation Error: {current_validation_error}\")\n",
    "                if self.previous_validation_error < current_validation_error:\n",
    "                    for layer in self.neural_network.layers:\n",
    "                        layer.restore_weights_and_biases()\n",
    "                    break\n",
    "                else:\n",
    "                    self.previous_validation_error = current_validation_error\n",
    "                    for layer in self.neural_network.layers:\n",
    "                        layer.save_weights_and_biases()\n",
    "        \n",
    "    \n",
    "    def forward_pass(self, inputs):\n",
    "        \"\"\"Performs forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "        inputs: Vector of values represneting a training example.\n",
    "        \"\"\"   \n",
    "        for i, layer in enumerate(self.neural_network.layers):\n",
    "            if i == 0:\n",
    "                layer.forward_pass(inputs)\n",
    "            else:\n",
    "                previous_layer = self.neural_network.layers[i-1]\n",
    "                layer.forward_pass(previous_layer.output)\n",
    "                \n",
    "        \n",
    "    def backward_pass(self, c, epochs_passed, learning_rate):\n",
    "        \"\"\"Performs backward pass through the network.\n",
    "\n",
    "        Args:\n",
    "        c: The label for the training example.\n",
    "        \"\"\"\n",
    "        reversed_layers = list(reversed(self.neural_network.layers))\n",
    "        for i, layer in enumerate(reversed_layers):\n",
    "            if i == 0:\n",
    "                # output layer backward pass\n",
    "                layer.delta = np.multiply(\n",
    "                    c - layer.output,\n",
    "                    layer.activation_function.vectorised_der(layer.sum)\n",
    "                )\n",
    "            else:\n",
    "                # hidden layer backward pass\n",
    "                next_layer = reversed_layers[i-1]\n",
    "                layer.delta = np.multiply(\n",
    "                    np.dot(next_layer.weights, next_layer.delta.T).T,\n",
    "                    layer.activation_function.vectorised_der(layer.sum)\n",
    "                )\n",
    "            \n",
    "    def update_weights(\n",
    "        self,\n",
    "        inputs: np.ndarray,\n",
    "        learning_rate: float\n",
    "    ):\n",
    "        \"\"\"Updates weights in the network.\n",
    "        \n",
    "        Args:\n",
    "        inputs: Inputs to the network.\n",
    "        learning_rate: Learning rate.\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.neural_network.layers):\n",
    "            #previous_weights = layer.weights.copy()\n",
    "            #previous_biases = layer.biases.copy()\n",
    "            if i == 0:\n",
    "                layer.weights = layer.weights + learning_rate * np.dot(inputs.T, layer.delta)\n",
    "            else:\n",
    "                previous_layer = self.neural_network.layers[i-1]\n",
    "                layer.weights = layer.weights + learning_rate * np.dot(\n",
    "                    previous_layer.output.T,\n",
    "                    layer.delta\n",
    "                )\n",
    "            layer.biases = layer.biases + learning_rate * layer.delta\n",
    "            #weights_delta = layer.weights - previous_weights\n",
    "            #biases_delta = layer.biases - previous_biases\n",
    "            #layer.weights = layer.weights + 0.9 * weights_delta\n",
    "            #layer.biases = layer.biases + 0.9 * biases_delta\n",
    "            \n",
    "    def bold_driver(\n",
    "        self,\n",
    "        current_training_error: float,\n",
    "        learning_rate: float\n",
    "    ):\n",
    "        \"\"\"Implements the Bold Driver extension.\"\"\"\n",
    "        if ((current_training_error / self.previous_training_error) * 100) >= 4:\n",
    "            # Decrease the learning rate if the error has increased.\n",
    "            learning_rate = learning_rate * 0.7\n",
    "            for layer in self.neural_network.layers:\n",
    "                layer.restore_weights_and_biases()\n",
    "            print(\"Decreased\")\n",
    "        elif ((current_training_error / self.previous_training_error) * 100) <= 96:\n",
    "            # Increase the learning rate if the error has decreased.\n",
    "            learning_rate = learning_rate * 1.05\n",
    "            print(\"Increased\")\n",
    "        # Check if the learning rate is the range.\n",
    "        if learning_rate < 0.01:\n",
    "            learning_rate = 0.01\n",
    "        elif learning_rate > 0.5:\n",
    "            learning_rate = 0.5\n",
    "        return learning_rate\n",
    "                \n",
    "            \n",
    "    def simulated_annealing(\n",
    "        self,\n",
    "        start_rate: float,\n",
    "        end_rate: float,\n",
    "        epoch_limit: int,\n",
    "        epochs_passed: int\n",
    "    ) -> float:\n",
    "        \"\"\"Returns annealed value of the learning rate.\n",
    "\n",
    "        Args:\n",
    "            start_rate: Initial learning rate value.\n",
    "            end_rate: Final learning rate value.\n",
    "            epoch_limit: Limit of epochs.\n",
    "            epochs_passed: Number of epochs that have elapsed.\n",
    "\n",
    "        Returns:\n",
    "            A float representing annealed learning rate.\n",
    "        \"\"\"\n",
    "        divisor = 1 + np.e ** (10 - (20 * epochs_passed) /  epoch_limit)\n",
    "        return end_rate + (start_rate - end_rate) * (1 - (1 / divisor))\n",
    "    \n",
    "    def weight_decay(\n",
    "        self,\n",
    "        epochs_passed: int,\n",
    "        learning_rate: float\n",
    "    ) -> float:\n",
    "        \"\"\"Calculates the penalty term for the error function.\n",
    "        \n",
    "        Args:\n",
    "            epochs_passed: Number of epochs that have elapsed.\n",
    "            learning_rate: Learning rate value.\n",
    "\n",
    "        Returns:\n",
    "            The penalty term for the error function.\n",
    "        \"\"\"\n",
    "        weights_and_biases_sum = 0\n",
    "        n = 0\n",
    "        for layer in self.neural_network.layers:\n",
    "            weights_and_biases_sum += np.sum(\n",
    "                np.power(\n",
    "                    np.vstack((layer.weights, layer.biases)),\n",
    "                    2\n",
    "                )\n",
    "            )\n",
    "            n += layer.weights.shape[0] * layer.weights.shape[1] + layer.biases.shape[1]\n",
    "        omega = (1 / (2 * n)) * weights_and_biases_sum\n",
    "        regularisation_parameter = 1 / (learning_rate * epochs_passed)\n",
    "        return regularisation_parameter * omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcc56a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Contains definition of a layer of a neural network.\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"Layer of a neural network.\n",
    "    \n",
    "    Attributes:\n",
    "        weights: set of weights of the layer.\n",
    "        biases: set of biases of the layer.\n",
    "        activation_function: Activation Function of the layer.\n",
    "        number_of_neurons: Number of neurons on the layer.\n",
    "        output: the most recent output of the layer.\n",
    "        saved_weights: Weights saved at previous validaton point.\n",
    "        saved_biases: Biases saved at previous validaton point.\n",
    "        delta: Delta values for neurons on the layer.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 number_of_inputs: int,\n",
    "                 number_of_neurons: int,\n",
    "                 activation_function\n",
    "                ):\n",
    "        \"\"\"Initialises a NeuralNetwork instance.\"\"\"\n",
    "        self.number_of_neurons = number_of_neurons\n",
    "        self.activation_function = activation_function\n",
    "        random_generator = np.random.default_rng(5)\n",
    "        low = -2 / number_of_inputs\n",
    "        high = 2 / number_of_inputs\n",
    "        self.weights = random_generator.uniform(\n",
    "            low=low,\n",
    "            high=high,\n",
    "            size=(number_of_inputs, number_of_neurons)\n",
    "        )\n",
    "        self.saved_weights = self.weights.copy()\n",
    "        self.biases = random_generator.uniform(\n",
    "            low=low,\n",
    "            high=high,\n",
    "            size=(1, number_of_neurons)\n",
    "        )\n",
    "        self.saved_biases = self.biases.copy()\n",
    "        self.delta = np.nan\n",
    "    \n",
    "    def forward_pass(self, inputs: np.ndarray):\n",
    "        \"\"\"Does the forward pass through the layer.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Inputs to the layer.\n",
    "        \"\"\"\n",
    "        self.sum = np.dot(inputs, self.weights) + self.biases\n",
    "        self.output = self.activation_function.vectorised_func(self.sum)\n",
    "            \n",
    "    def save_weights_and_biases(self):\n",
    "        \"\"\"Saves current weights and biases to keep them after updating.\n",
    "        \"\"\"\n",
    "        self.saved_weights = self.weights.copy()\n",
    "        self.saved_biases = self.biases.copy()\n",
    "        \n",
    "    def restore_weights_and_biases(self):\n",
    "        \"\"\"Restores saved weights and biases.\n",
    "        \"\"\"\n",
    "        self.weights = self.saved_weights\n",
    "        self.biases = self.saved_biases     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92fbb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Contains NeuralNetwork class definition.\n",
    "\n",
    "Run after running the Data Preprocessing notebook.\n",
    "\"\"\"\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"A neural network with single hidden layer and single node on the output layer.\n",
    "    \n",
    "    Attributes:\n",
    "        number_of_inputs: Number of inputs to the network.\n",
    "        layers: List of network's hidden layers and the output layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        number_of_inputs: int,\n",
    "        network_architecture,\n",
    "    ):\n",
    "        \"\"\"Initialises a NeuralNetwork instance.\n",
    "        \"\"\"\n",
    "        self.number_of_inputs = number_of_inputs\n",
    "        self.layers = []\n",
    "        for i, item in enumerate(network_architecture):\n",
    "            if i == 0:\n",
    "                number_of_layer_inputs = number_of_inputs\n",
    "            else:\n",
    "                number_of_layer_inputs = self.layers[-1].number_of_neurons\n",
    "            layer = Layer(\n",
    "                number_of_inputs=number_of_layer_inputs,\n",
    "                number_of_neurons=item[0],\n",
    "                activation_function=item[1]\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        training_set: np.ndarray,\n",
    "        validation_set: np.ndarray,\n",
    "        validation_frequency: int,\n",
    "        learning_rate: float,\n",
    "        epoch_limit: int,\n",
    "        training_algorithm\n",
    "    ):\n",
    "        \"\"\"Trains NeuralNetwork instance.\n",
    "        \n",
    "        Args:\n",
    "        training_set: Set that instance is trained on.\n",
    "        validation_set: Set the instance is tested on during training.\n",
    "        validation_frequency: Frequency of testing on validation set.\n",
    "            Expressed in epochs.\n",
    "        learning_rate: Learning rate.\n",
    "        epoch_limit: The maximum number of epochs to train the instace for.\n",
    "        \"\"\"\n",
    "        training_algorithm.train(\n",
    "            training_set=training_set,\n",
    "            validation_set=validation_set,\n",
    "            validation_frequency=validation_frequency,\n",
    "            learning_rate=learning_rate,\n",
    "            epoch_limit=epoch_limit\n",
    "        )   \n",
    "                    \n",
    "    def test(self, test_set) -> list:\n",
    "        \"\"\"Tests the neural network and returns both observed and modelled values.\n",
    "        \n",
    "        Args:\n",
    "            test_set: Array of test examples (including labels).\n",
    "        Returns: two-element list with observed values being the first\n",
    "            element and modelled being the second.\n",
    "        \n",
    "        \"\"\"\n",
    "        predicted_values = np.empty(shape=(test_set.shape[0], 1))\n",
    "        correct_values = test_set[:, test_set.shape[1] - 1]\n",
    "        correct_values = correct_values.reshape(-1, 1)\n",
    "        for i in range(len(test_set)):\n",
    "            # Split individual examples into inputs (item) and label (c).\n",
    "            item, c = np.hsplit(test_set[i], [test_set.shape[1] - 1])\n",
    "            item = item.reshape(1, -1)\n",
    "            predicted_values[i] = self.predict(item)\n",
    "        correct_values = destandardise(correct_values, max_value=max_value, min_value=min_value)\n",
    "        predicted_values = destandardise(predicted_values, max_value=max_value, min_value=min_value)\n",
    "        return [correct_values, predicted_values]\n",
    "       \n",
    "    def save_network(self, file_path: str):\n",
    "        \"\"\"Saves the network to the file in JSON format.\n",
    "        \n",
    "        Args:\n",
    "        file_path: File path to the target file.\n",
    "        \"\"\"\n",
    "        layers_details = []\n",
    "        for layer in self.layers:\n",
    "            layer_details = {\n",
    "                \"weights\": layer.weights.tolist(),\n",
    "                \"biases\": layer.biases.tolist(),\n",
    "                \"activation_function\": layer.activation_function.__class__.__name__\n",
    "            }\n",
    "            layers_details.append(layer_details)\n",
    "\n",
    "        neural_network = {\n",
    "            \"layers\": layers_details\n",
    "        }\n",
    "        try:\n",
    "            with open(file_path, \"w\") as f:\n",
    "                json.dump(neural_network, f)\n",
    "        except IOError as e:\n",
    "            print(e)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected exception: {e}\")\n",
    "            \n",
    "    @classmethod\n",
    "    def load_network(cls, file_path: str):\n",
    "        \"\"\"Returns the network loaded from the file in JSON format.\n",
    "        \n",
    "        Args:\n",
    "        file_path: File path to the target file.\n",
    "        \n",
    "        Returns:\n",
    "        NeuralNetwork instance.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, \"r\") as f:\n",
    "                nn_details = json.load(f)\n",
    "            layers = []\n",
    "            for layer_details in nn_details['layers']:\n",
    "                weights = np.array(layer_details['weights'])\n",
    "                biases = np.array(layer_details['biases'])\n",
    "                layer = Layer(\n",
    "                    number_of_inputs=weights.shape[0],\n",
    "                    number_of_neurons=weights.shape[1],\n",
    "                    activation_function=eval(f\"{layer_details['activation_function']}()\")\n",
    "                )\n",
    "                layer.weights = weights\n",
    "                layer.biases = biases\n",
    "                layers.append(layer)\n",
    "        except IOError as e:\n",
    "            print(e)\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected exception: {e}\")\n",
    "            return None\n",
    "        architecture = []\n",
    "        for layer in layers:\n",
    "            layer_details = tuple([layer.weights.shape[1], layer.activation_function])\n",
    "            architecture.append(layer_details)\n",
    "        neural_network = cls(\n",
    "            number_of_inputs=layers[0].weights.shape[0],\n",
    "            network_architecture=architecture\n",
    "        )\n",
    "        neural_network.layers = layers\n",
    "        return neural_network\n",
    "                \n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        \"\"\"Predicts value for given predictor values.\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i == 0:\n",
    "                layer.forward_pass(inputs)\n",
    "            else:\n",
    "                previous_layer = self.layers[i-1]\n",
    "                layer.forward_pass(previous_layer.output)\n",
    "        return self.layers[-1].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4c5c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load min and max values for the destandardisation process.\n",
    "with open(\"standardisation.json\", \"r\") as f:\n",
    "    min_max_values = json.load(f)\n",
    "\n",
    "min_value = min_max_values[\"min\"]\n",
    "max_value = min_max_values[\"max\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048928b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train.\n",
    "\n",
    "training_set = pd.read_csv(\"data/training-set.csv\")\n",
    "training_set = training_set.to_numpy() # Convert to a numpy array.\n",
    "training_set = training_set[:, 1:] # Get rid of the index column.\n",
    "\n",
    "validation_set = pd.read_csv(\"data/validation-set.csv\")\n",
    "validation_set = validation_set.to_numpy() # Convert to a numpy array.\n",
    "validation_set = validation_set[:, 1:] # Get rid of the index column.\n",
    "\n",
    "test_set = pd.read_csv(\"data/test-set.csv\")\n",
    "test_set = test_set.to_numpy() # Convert to a numpy array.\n",
    "test_set = test_set[:, 1:] # Get rid of the index column.\n",
    "\n",
    "\n",
    "neural_network = NeuralNetwork(\n",
    "    number_of_inputs=training_set.shape[1]-1,\n",
    "    network_architecture=[[6, Sigmoid()], [1, Sigmoid()]]\n",
    ")\n",
    "\n",
    "backpropagation = Backpropagation(\n",
    "    neural_network=neural_network\n",
    ")\n",
    "    \n",
    "start_time = time.perf_counter()\n",
    "neural_network.train(\n",
    "    training_set=training_set,\n",
    "    validation_set=validation_set,\n",
    "    validation_frequency=10,\n",
    "    learning_rate=0.2,\n",
    "    epoch_limit=10000,\n",
    "    training_algorithm=backpropagation\n",
    ")\n",
    "#end_time = time.perf_counter()\n",
    "#print(f\"Training time: {end_time - start_time} seconds\")\n",
    "#print(f\"Smallest Validation Error: {backpropagation.previous_validation_error}\")\n",
    "#print(f\"Number of epochs: {backpropagation.epochs}\")\n",
    "#observed, predicted = neural_network.test(test_set)\n",
    "#print(f\"{mse(observed, predicted)},\")\n",
    "#print(f\"{rmse(observed, predicted)},\")\n",
    "#print(f\"{msre(observed, predicted)},\")\n",
    "#print(f\"{ce(observed, predicted)},\")\n",
    "#print(f\"{rsqr(observed, predicted)}\\n\")\n",
    "\n",
    "#neural_network.save_network(\"sigmoid-6-0.2-sa.json\")\n",
    "    \n",
    "#neural_network2 = NeuralNetwork.load_network(\"sigmoid-6-0.2-sa.json\")\n",
    "    \n",
    "#print(f\"Hidden weights are equal? {np.array_equal(neural_network.hidden_layer.weights, neural_network2.hidden_layer.weights)}\")\n",
    "#print(f\"Hidden biases are equal? {np.array_equal(neural_network.hidden_layer.biases, neural_network2.hidden_layer.biases)}\")\n",
    "#print(f\"Output weights are equal? {np.array_equal(neural_network.output_layer.weights, neural_network2.output_layer.weights)}\")\n",
    "#print(f\"Output biases are equal? {np.array_equal(neural_network.output_layer.biases, neural_network2.output_layer.biases)}\")\n",
    "\n",
    "#print(f\"{neural_network.activation_function} = {neural_network2.activation_function}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea2f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "with open(\"sigmoid-6-0.2-sa.csv\", \"w\") as f:\n",
    "    f.write(f\"{neural_network.activation_function.__class__.__name__},\")\n",
    "    f.write(f\"{neural_network.hidden_nodes},\")\n",
    "    f.write(f\"{0.4},\")\n",
    "    f.write(f\"{end_time - start_time},\")\n",
    "    f.write(f\"{neural_network.epochs},\")\n",
    "    f.write(f\"{neural_network.previous_validation_error},\")\n",
    "    observed, predicted = neural_network.test(test_set)\n",
    "    f.write(f\"{mse(observed, predicted)},\")\n",
    "    f.write(f\"{rmse(observed, predicted)},\")\n",
    "    f.write(f\"{msre(observed, predicted)},\")\n",
    "    f.write(f\"{ce(observed, predicted)},\")\n",
    "    f.write(f\"{rsqr(observed, predicted)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ba734f",
   "metadata": {},
   "source": [
    "with open(\"neural-network-configs.csv\", \"a\") as f:\n",
    "    activation_functions = [Sigmoid(), Tanh(), Relu(), LeakyRelu()]\n",
    "    hidden_nodes = range((training_set.shape[1] - 1) // 2, 2 * training_set.shape[1] - 1)\n",
    "    learning_rates = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    for activation_function in activation_functions:\n",
    "        for n_hidden_nodes in hidden_nodes:\n",
    "            for learning_rate in learning_rate:\n",
    "                neural_network = NeuralNetwork(\n",
    "                    number_of_inputs=training_set.shape[1]-1,\n",
    "                    network_architecture=[[n_hidden_nodes, activation_function], [1, activation_function]]\n",
    "                )            \n",
    "                start_time = time.perf_counter()\n",
    "                neural_network.train(\n",
    "                    training_set=training_set,\n",
    "                    validation_set=validation_set,\n",
    "                    validation_frequency=10,\n",
    "                    learning_rate=learning_rate,\n",
    "                    epoch_limit=500\n",
    "                )\n",
    "                end_time = time.perf_counter()\n",
    "                f.write(f\"{activation_function.__class__.__name__},\")\n",
    "                f.write(f\"{n_hidden_nodes},\")\n",
    "                f.write(f\"{learning_rate},\")\n",
    "                f.write(f\"{end_time - start_time},\")\n",
    "                f.write(f\"{neural_network.epochs},\")\n",
    "                f.write(f\"{neural_network.previous_validation_error},\")\n",
    "                observed, predicted = neural_network.test(validation_set)\n",
    "                f.write(f\"{mse(observed, predicted)},\")\n",
    "                f.write(f\"{rmse(observed, predicted)},\")\n",
    "                f.write(f\"{msre(observed, predicted)},\")\n",
    "                f.write(f\"{ce(observed, predicted)},\")\n",
    "                f.write(f\"{rsqr(observed, predicted)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a888653a",
   "metadata": {},
   "source": [
    "with open(\"neural-network-configs-selected-validation-set.csv\", \"w\") as f:\n",
    "    configs = [\n",
    "        [Sigmoid(), 5, 0.2],\n",
    "        [Sigmoid(), 6, 0.2],\n",
    "        [LeakyRelu(), 14, 0.6],\n",
    "        [LeakyRelu(), 14, 0.2],\n",
    "        [Relu(), 9, 0.2],\n",
    "        [Tanh(), 15, 0.1]\n",
    "    ]\n",
    "    for config in configs:\n",
    "        neural_network = NeuralNetwork(\n",
    "            number_of_inputs=training_set.shape[1]-1,\n",
    "            network_architecture=[[config[1], config[0]], [1, config[0]]]\n",
    "        )\n",
    "        backpropagation = Backpropagation(\n",
    "            neural_network=neural_network\n",
    "        )\n",
    "        start_time = time.perf_counter()\n",
    "        neural_network.train(\n",
    "            training_set=training_set,\n",
    "            validation_set=validation_set,\n",
    "            validation_frequency=10,\n",
    "            learning_rate=config[2],\n",
    "            epoch_limit=100000,\n",
    "            training_algorithm=backpropagation\n",
    "        )\n",
    "        end_time = time.perf_counter()\n",
    "        f.write(f\"{config[0].__class__.__name__},\")\n",
    "        f.write(f\"{config[1]},\")\n",
    "        f.write(f\"{config[2]},\")\n",
    "        f.write(f\"{end_time - start_time},\")\n",
    "        f.write(f\"{backpropagation.epochs},\")\n",
    "        f.write(f\"{backpropagation.previous_validation_error},\")\n",
    "        observed, predicted = neural_network.test(validation_set)\n",
    "        f.write(f\"{mse(observed, predicted)},\")\n",
    "        f.write(f\"{rmse(observed, predicted)},\")\n",
    "        f.write(f\"{msre(observed, predicted)},\")\n",
    "        f.write(f\"{ce(observed, predicted)},\")\n",
    "        f.write(f\"{rsqr(observed, predicted)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6372a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "df = pd.DataFrame({\n",
    "   'Training Error': backpropagation.training_errors,\n",
    "   'Validation Error': backpropagation.validation_errors\n",
    "   }, index = backpropagation.tested_at_epochs)\n",
    "lines = df.plot(kind=\"line\", xlabel=\"Epoch\", ylabel=\"MSE\")\n",
    "plt.savefig('figures/training-validation-error.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a3de7c",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb383da",
   "metadata": {},
   "source": [
    "## Batch Learning\n",
    "\n",
    "Batch size may improve efficiency. Showing all sampes at once can cause overfitting. It will be bad at generalsing.\n",
    "\n",
    "Typical batch size: 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17560591",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "          [0.5, -.91, 0.26, -0.5],\n",
    "          [-0.26, -.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "\n",
    "weights2 = [[0.1, -0.14, 0.5],\n",
    "          [-0.5, 0.12, -0.33],\n",
    "          [-0.44, 0.73, -0.13]]\n",
    "\n",
    "biases2 = [-1, 2, -0.5]\n",
    "\n",
    "layer1_outputs = np.dot(inputs, np.array(weights).T) + biases\n",
    "\n",
    "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n",
    "\n",
    "print(layer2_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056ce84b",
   "metadata": {},
   "source": [
    "## Feature Data Set\n",
    "\n",
    "Feature data set is usaully denoted with `X`.\n",
    "\n",
    "Labels are usually denoted with `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bbc625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
