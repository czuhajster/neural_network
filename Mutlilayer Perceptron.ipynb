{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534af2af",
   "metadata": {},
   "source": [
    "# Backpropagation Neural Network\n",
    "\n",
    "Backpropagation directs the weight changes down the gradient of steepest descent (hence Gradient Descent) of the error function and adjusts the weights according to a learning parameter which is set by the user.\n",
    "\n",
    "This is possible because of the continuous nature and differentiability of the activation functions in the neurons.\n",
    "\n",
    "## Momentum\n",
    "\n",
    "## Bold Driver\n",
    "\n",
    "## Annealing\n",
    "\n",
    "## Weight Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd018c2",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "\n",
    "Gradient of the error function\n",
    "\n",
    "- $\\frac{\\delta E}{\\delta w_{ij}}$ is the rate of change of Error $E$ with respect to a weight $w_{ij}$. This is the gradient of the error function.\n",
    "\n",
    "## Updating weights\n",
    "\n",
    "When updating weights, we are **subtracting** the gradient of the error function ($\\times \\rho$):\n",
    "\n",
    "$$\n",
    "w^*_{i,j} = w_{i,j} + \\rho \\delta_j u_i\n",
    "$$\n",
    "\n",
    "\n",
    "![Gradient of the Error function](figures/error-gradient.png)\n",
    "\n",
    "\n",
    "Too small weights - stuck in local minima\n",
    "\n",
    "- we start with random weights and biases\n",
    "\n",
    "- searching multidiensional spaces result less often in being caught in local minima.\n",
    "\n",
    "## When do we stop learning?\n",
    "\n",
    "\n",
    "- We tend to stop learning when the error of an independent validation set increases.\n",
    "\n",
    "- Every $x$ number of epochs, we test it against this unseen validation set.\n",
    "\n",
    "\n",
    "### Symmetry in Weight Space\n",
    "\n",
    "Network with $M$ hidden nodes exhbits symetry by a factor of $M!2^M$.\n",
    "\n",
    "For example, if we have 3 hidden nodes, we will have $3!2^3 = 48$ the same global minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c53ee",
   "metadata": {},
   "source": [
    "## Measuring Performance\n",
    "\n",
    "### Mean Squared Error\n",
    "\n",
    "$$\n",
    "MSE = \\frac{\\sum(O - M)^2}{n}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $O$ is the observed value\n",
    "- $M$ is the modelled value\n",
    "- $n$ is the number of example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd9ea93",
   "metadata": {},
   "source": [
    "## Number of Hidden Nodes\n",
    "\n",
    "- It is not fixed\n",
    "- The general rule of thumb is, having $n$ inputs, to try $\\frac{n}{2}$ to $2n$ hidden nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29540ff1",
   "metadata": {},
   "source": [
    "## Things to manipulate\n",
    "\n",
    "- number of hidden nodes\n",
    "- step size\n",
    "- activation function (optionally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9b98555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ee7cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron():\n",
    "    \"\"\"Object representing perceptron with two inputs.\n",
    "\n",
    "    Attributes:\n",
    "        e: A training set.\n",
    "        w0: Bias weight.\n",
    "        w1: The weight of the first input.\n",
    "        w2: The weight of the second input.\n",
    "        epochs: Number of epochs before stabilisation.\n",
    "    \"\"\"\n",
    "    def __init__(self, e, activation_function = lambda s: 1 if s > 0 else -1):\n",
    "        '''Initialises Perceptron object.'''\n",
    "        self.w0 = 0\n",
    "        self.w1 = 0\n",
    "        self.w2 = 0\n",
    "        self.e = e\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Trains perceptron.\"\"\"\n",
    "        self.epochs = 1\n",
    "        stable = False\n",
    "        while not stable:\n",
    "            stable = True\n",
    "            for example in self.e:\n",
    "                print(example)\n",
    "                if self.classify(example[1], example[2]) == example[3]:\n",
    "                    pass\n",
    "                else:\n",
    "                    self.w0 += example[3] * example[0]\n",
    "                    self.w1 += example[3] * example[1]\n",
    "                    self.w2 += example[3] * example[2]\n",
    "                    stable = False\n",
    "            if not stable:\n",
    "                self.epochs += 1\n",
    "\n",
    "    def classify(self, x1, x2):\n",
    "        \"\"\"Classifies an object.\"\"\"\n",
    "        s = (self.w1 * x1) + (self.w2 * x2) + self.w0\n",
    "        return self.activation_function(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94222063",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "- Every node on hidden layers and output layer has an activation function.\n",
    "\n",
    "\n",
    "\n",
    "- We might have **linear activation function in the output layer** instead of using sigmoid or tanh.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c951ed81",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "#### First Order Derivative\n",
    "\n",
    "We also need to calculate the first order differential of the function.\n",
    "\n",
    "$$\n",
    "f'(x) = f(x)(1 - f(x))\n",
    "$$\n",
    "\n",
    "$$\n",
    "f'(S_j) = u_j (1 - u_j)\n",
    "$$\n",
    "\n",
    "#### Delta Values\n",
    "\n",
    "- $\\delta_j = (C - u_O) f'(S_O)$ O is the output node.\n",
    "- $\\delta_j = w_{j,O} \\delta_O f'(S_j)$ for hidden layer nodes.\n",
    "\n",
    "\n",
    "\n",
    "#### Notes\n",
    "\n",
    "Sigmoid func has gradient problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a10c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"Represents Sigmoid activation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialises a sigmoid object.\n",
    "        \"\"\"\n",
    "        self.vectorised_func = np.vectorize(self.func)\n",
    "        self.vectorised_der = np.vectorize(self.der)\n",
    "        \n",
    "    def func(self, x):\n",
    "        \"\"\"Calculates output of the Sigmoid function.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.e ** (-x))\n",
    "    \n",
    "    def der(self, x):\n",
    "        \"\"\"Calculates output of the derivative of the Sigmoid function.\n",
    "        \"\"\"\n",
    "        return self.func(x) * (1 - self.func(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be4c0f",
   "metadata": {},
   "source": [
    "### Tanh\n",
    "\n",
    "$$\n",
    "tanh x = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "#### First Order Derivative\n",
    "\n",
    "$$\n",
    "tanh' x = 1 - tanh^2 x\n",
    "$$\n",
    "\n",
    "$$\n",
    "f'(S_j) = 1 - u^2_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264882be",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dcc56a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Contains definition of a layer of a neural network.\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"Layer of a neural network.\n",
    "    \n",
    "    Attributes:\n",
    "        weights: set of weights of the layer.\n",
    "        biases: set of biases of the layer.\n",
    "        activation_function: Activation Function of the layer.\n",
    "        number_of_inputs: Number of inputs coming to the layer.\n",
    "        number_of_inputs: Number of inputs coming to the layer.\n",
    "        output: the most recent output of the layer.\n",
    "        previous_weights: Previous weights.\n",
    "        previous_biases: Previous biases.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 number_of_inputs: int,\n",
    "                 number_of_neurons: int,\n",
    "                 activation_function\n",
    "                ):\n",
    "        \"\"\"Initialises a NeuralNetwork instance.\n",
    "        \"\"\"\n",
    "        self.number_of_neurons = number_of_neurons\n",
    "        self.activation_function = activation_function\n",
    "        random_generator = np.random.default_rng(5)\n",
    "        #NNFS self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        low = -2 / number_of_inputs\n",
    "        high = 2 / number_of_inputs\n",
    "        self.weights = random_generator.uniform(\n",
    "            low=low,\n",
    "            high=high,\n",
    "            size=(number_of_inputs, number_of_neurons)\n",
    "        )\n",
    "        self.previous_weights = self.weights.copy()\n",
    "        self.biases = random_generator.uniform(\n",
    "            low=low,\n",
    "            high=high,\n",
    "            size=(1, number_of_neurons)\n",
    "        )\n",
    "        self.previous_biases = self.biases.copy()\n",
    "        self.delta = np.nan\n",
    "    \n",
    "    def forward_pass(self, inputs: np.ndarray):\n",
    "        \"\"\"Does the forward pass through the layer.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Inputs to the layer.\n",
    "        \"\"\"\n",
    "        self.sum = np.dot(inputs, self.weights) + self.biases\n",
    "        self.output = self.activation_function.vectorised_func(self.sum)\n",
    "        \n",
    "    def update_weights(self, learning_parameter, inputs):\n",
    "        \"\"\"Updates the layer's weights.\n",
    "        \n",
    "        Args:\n",
    "            learning_parameter: Learning parameter of the network.\n",
    "            inputs: Inputs to the layer.\n",
    "        \"\"\"\n",
    "        self.weights = self.weights + learning_parameter * np.dot(inputs.T, self.delta)\n",
    "        self.biases = self.biases + learning_parameter * self.delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab2d1f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(Layer):\n",
    "    \"\"\"Hidden Layer of a neural network.\n",
    "    \n",
    "    Attributes:\n",
    "        weights: set of weights of the layer.\n",
    "        biases: set of biases of the layer.\n",
    "        activation_function: Activation Function of the layer.\n",
    "        number_of_inputs: Number of inputs coming to the layer.\n",
    "        number_of_neurons: Number of neurons in the layer.\n",
    "        output: the most recent output of the layer.\n",
    "        delta: Delta value (output of the backward pass) of the layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def backward_pass(self, output_weights, output_delta):\n",
    "        \"\"\"Does the backward pass through the layer.\n",
    "        \n",
    "        Args:\n",
    "            y: Correct output (label) of the training example.\n",
    "        \"\"\"\n",
    "        self.delta = output_delta * np.multiply(self.activation_function.vectorised_der(self.sum), output_weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "433549a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer(Layer):\n",
    "    \"\"\"Single-node output layer of a neural network.\n",
    "    \n",
    "    Attributes:\n",
    "        weights: set of weights of the layer.\n",
    "        biases: set of biases of the layer.\n",
    "        activation_function: Activation Function of the layer.\n",
    "        number_of_inputs: Number of inputs coming to the layer.\n",
    "        number_of_neurons: Number of neurons in the layer.\n",
    "        output: the most recent output of the layer.\n",
    "        delta: Delta value (output of the backward pass) of the layer.\n",
    "    \"\"\"\n",
    "        \n",
    "    def backward_pass(self, y):\n",
    "        \"\"\"Does the backward pass through the layer.\n",
    "        \n",
    "        Args:\n",
    "            y: Correct output (label) of the training example.\n",
    "        \"\"\"\n",
    "        self.delta = (y - self.output) * self.activation_function.vectorised_der(self.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e92fbb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Contains NeuralNetwork class definition.\n",
    "\n",
    "Run after running the Data Preprocessing notebook.\n",
    "\"\"\"\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"A neural network with single hidden layer and single node on outputlayer.\n",
    "    \n",
    "    Attributes:        \n",
    "        training_set: Set that instance is to be trained on.\n",
    "        test_set: Set that instance is to be tested on.\n",
    "        hidden_nodes: Number of nodes on the hidden layer.\n",
    "        activation_function: Activation function used for\n",
    "            the neural network.\n",
    "        learning_parameter: Step size parameter.\n",
    "        epochs: Number of epochs the nerual network has gone through during the last training.\n",
    "        validation_frequency: Frequency of testing on validation set expressed in epochs.\n",
    "        previous_validation_error: Previous validation error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        training_set: np.ndarray,\n",
    "        test_set: np.ndarray,\n",
    "        hidden_nodes: int,\n",
    "        activation_function,\n",
    "        epoch_limit,\n",
    "        validation_frequency,\n",
    "        learning_parameter: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"Initialises a NeuralNetwork instance.\n",
    "        \"\"\"\n",
    "        # Input is a matrix where each row is one instance\n",
    "        self.training_set = training_set\n",
    "        self.test_set = test_set\n",
    "        self.learning_parameter = learning_parameter\n",
    "        self.number_of_inputs = self.training_set.shape[1] - 1\n",
    "        self.hidden_layer = HiddenLayer(\n",
    "            self.number_of_inputs,\n",
    "            hidden_nodes,\n",
    "            activation_function\n",
    "        )\n",
    "        self.output_layer = OutputLayer(\n",
    "            self.hidden_layer.number_of_neurons,\n",
    "            1,\n",
    "            activation_function\n",
    "        )\n",
    "        self.epoch_limit = epoch_limit\n",
    "        self.validation_frequency = validation_frequency\n",
    "        self.epochs = 0\n",
    "        self.previous_validation_error = np.inf\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Trains NeuralNetwork instance.\n",
    "        \"\"\"\n",
    "        # Loop through the training set\n",
    "        for i in range(self.epoch_limit):\n",
    "            self.epochs = i + 1\n",
    "            for training_example in self.training_set:\n",
    "                # Split individual examples into inputs (item) and label (c).\n",
    "                item, c = np.hsplit(training_example, [self.training_set.shape[1] - 1])\n",
    "                item = item.reshape(1, -1)\n",
    "                self.forward_pass(item)\n",
    "                self.backward_pass(c)\n",
    "                self.update_weights(item)\n",
    "            # Test on validation set.\n",
    "            if (i + 1) % self.validation_frequency == 0:\n",
    "                # TODO: Test on the validation set.\n",
    "                \"\"\"\n",
    "                current_validation_error = self.train(self.validation_set)\n",
    "                if self.previous_validation_error < current_validation_error:\n",
    "                    break\n",
    "                else:\n",
    "                    self.validation_error = current_validation_error\n",
    "                \"\"\"\n",
    "                pass\n",
    "            \n",
    "            \n",
    "    def forward_pass(self, inputs):\n",
    "        \"\"\"Performs forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "        inputs: Vector of values represneting a training example.\n",
    "        \"\"\"\n",
    "        self.hidden_layer.forward_pass(inputs)\n",
    "        self.output_layer.forward_pass(self.hidden_layer.output)       \n",
    "    \n",
    "    def backward_pass(self, c):\n",
    "        \"\"\"Performs backward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "        c: The label for the training example.\n",
    "        \"\"\"\n",
    "        self.output_layer.backward_pass(c)\n",
    "        self.hidden_layer.backward_pass(\n",
    "            self.output_layer.weights,\n",
    "            self.output_layer.delta\n",
    "        )\n",
    "        \n",
    "    def update_weights(self, inputs):\n",
    "        \"\"\"Update weights in the network.\n",
    "        \"\"\"\n",
    "        self.hidden_layer.update_weights(self.learning_parameter, inputs)\n",
    "        self.output_layer.update_weights(self.learning_parameter, self.hidden_layer.output)\n",
    "        \n",
    "        \n",
    "    def test(self, test_set):\n",
    "        \"\"\"Tests NeuralNetwork instance.\n",
    "        \"\"\"\n",
    "        predicted_values = np.empty(shape=(test_set.shape[0], 1))\n",
    "        correct_values = test_set[:, test_set.shape[1] - 1]\n",
    "        for i in range(len(test_set)):\n",
    "                # Split individual examples into inputs (item) and label (c).\n",
    "                item, c = np.hsplit(test_set[i], [test_set.shape[1] - 1])\n",
    "                item = item.reshape(1, -1)\n",
    "                self.forward_pass(item)\n",
    "                predicted_values[i] = self.output_layer.output\n",
    "        # Calculate MSE.        \n",
    "        result = np.sum(np.power(correct_values - predicted_values)) / test_set.shape[0]\n",
    "        return result\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        \"\"\"Predicts value for given predictor values.\n",
    "        \"\"\"\n",
    "        self.hidden_layer.forward_pass(inputs)\n",
    "        self.output_layer.forward_pass(self.hidden_layer.output)\n",
    "        return self.output_layer.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a3de7c",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9048928b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ae14c5968cff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlearning_parameter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mneural_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-159a20594346>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtraining_example\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;31m# Split individual examples into inputs (item) and label (c).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mhsplit\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mhsplit\u001b[0;34m(ary, indices_or_sections)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_or_sections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_or_sections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msplit\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(ary, indices_or_sections, axis)\u001b[0m\n\u001b[1;32m    872\u001b[0m             raise ValueError(\n\u001b[1;32m    873\u001b[0m                 'array split does not result in an equal division') from None\n\u001b[0;32m--> 874\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_or_sections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36marray_split\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36marray_split\u001b[0;34m(ary, indices_or_sections, axis)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     \u001b[0msub_arys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m     \u001b[0msary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNsections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiv_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mswapaxes\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mswapaxes\u001b[0;34m(a, axis1, axis2)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m     \"\"\"\n\u001b[0;32m--> 595\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'swapaxes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train.\n",
    "\n",
    "training_set = pd.read_csv(\"data/training-set.csv\")\n",
    "training_set = training_set.to_numpy() # Convert to a numpy array.\n",
    "training_set = training_set[:, 1:] # Get rid of the index column.\n",
    "\n",
    "neural_network = NeuralNetwork(\n",
    "    training_set=training_set,\n",
    "    test_set=None,\n",
    "    hidden_nodes=9,\n",
    "    activation_function = Sigmoid(),\n",
    "    epoch_limit=1000,\n",
    "    validation_frequency=500,\n",
    "    learning_parameter=0.1 \n",
    ")\n",
    "neural_network.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb383da",
   "metadata": {},
   "source": [
    "## Batch Learning\n",
    "\n",
    "Batch size may improve efficiency. Showing all sampes at once can cause overfitting. It will be bad at generalsing.\n",
    "\n",
    "Typical batch size: 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17560591",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "          [0.5, -.91, 0.26, -0.5],\n",
    "          [-0.26, -.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "\n",
    "weights2 = [[0.1, -0.14, 0.5],\n",
    "          [-0.5, 0.12, -0.33],\n",
    "          [-0.44, 0.73, -0.13]]\n",
    "\n",
    "biases2 = [-1, 2, -0.5]\n",
    "\n",
    "layer1_outputs = np.dot(inputs, np.array(weights).T) + biases\n",
    "\n",
    "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n",
    "\n",
    "print(layer2_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056ce84b",
   "metadata": {},
   "source": [
    "## Feature Data Set\n",
    "\n",
    "Feature data set is usaully denoted with `X`.\n",
    "\n",
    "Labels are usually denoted with `y`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
